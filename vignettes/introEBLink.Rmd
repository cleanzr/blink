---
title: "Introduction to blink"
author: "Rebecca C. Steorts"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Partitions}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

We present a small example from "Entity Resolution with Emprically Motivated Priors", Bayesian Analysis, (10),4:849-975. We will be using the RecordLinkage package in R and the RLdata500 data set. 

The blink package removes duplicate entries from multiple databases using the method outlined in the paper above. We illustrate an example of using this package using a German dataset comprised of first and last name and full date of birth.

Our goals include

- Presenting the RLdata500 dataset with summary information.
- Illustrating how we can format the RLdata500 dataset to work with the blink package
- Examples for setting the tuning parameters for blink
- Running the Gibbs sampler in blink
- Illustrating a few plots and sample output

## Understanding the RLdata500 dataset

The RLdata500 data set exists already in the RecordLinkage package in R. We review this data set for the user. 

The RLdata500 data consists of 500 records with 10 percent duplication. Thus, there are 450 unique individuals. There is full information on each record containing first name, last name, and full date of birth. 

We first load the Record Linkgae package and load the RLdata500 data set. We also, provide the first few lines of the data. 

```{r, echo=TRUE, message=FALSE, knitr::opts_chunk$set(cache=TRUE)}
library(blink)
data(RLdata500)
head(RLdata500)
```

## Preparing the data

Next, we prepare the data for working with the blink package. 

```{r}
# X.c contains the categorical variables
# X.s contains the string variables 
# p.c is the number of categorical variables
# p.s contains the number of string variables
X.c <- RLdata500[c("by","bm","bd")]
X.c <- as.matrix(RLdata500[,"bd"],ncol=1)
p.c <- ncol(X.c)
X.s <- as.matrix(RLdata500[-c(2,4,7)])
p.s <- ncol(X.s)
```

## Setting the Tuning Parameters

Now, we give a small example for setting the tuning parameters before running the Gibbs sampler.

First, we work with a file number identifier.
```{r}
# File number identifier
# Note: Recall that X.c and X.s include all files "stacked" on top of each other.
# The vector below keeps track of which rows of X.c and X.s are in which files.
file.num <- rep(c(1,2,3),c(200,150,150))
```

Next, we work with the parameters that tune the prior on the amount of distortion that goes into the model. 
```{r}
# Subjective choices for distortion probability prior
a <-1
b <- 999
```

Then we write a function for the Edit distance between two strings. Other distance functions could be used, such as Jaro-Winkler.  
```{r}
d <- function(string1,string2){adist(string1,string2)}
```

For the steepness parameter, we recommend
```{r}
c <- 1
```

## The Gibbs Sampler for bLink
We now run a test version of the Gibbs sampler using blink, with 10 Gibbs iterations and a maximum size of M=500 (assuming the overall known population size is 500).

```{r,results="hide"}
library(knitr)
library(blink)
library(plyr)
Sys.setenv(TMPDIR="/tmp/")
configure.vars="TMPDIR=/tmp/"
lam.gs <- rl.gibbs(file.num=file.num,X.s=X.s,X.c=X.c,num.gs=2,a=a,b=b,c=c,d=d, M=500)
#system.time(lam.gs <- rl.gibbs(file.num=file.num,X.s=X.s,X.c=X.c,num.gs=2,a=a,b=b,c=c,d=d, M=500))
```

## Summary information of blink

Let's read in the estimate linkage structure using 10 Gibbs iterations.

```{r, fig.show="hold", fig.cap="The red line is the ground truth (450), which is not close to the estimate (500) since we only ran 10 Gibbs sampling iterations."}
#estLink <- tempfile(pattern = "lam.gs")
estLink <- lam.gs
estPopSize <- apply(estLink , 1, function(x) {length(unique(x))})
plot(density(estPopSize),xlim=c(300,500),main="",lty=1, "Observed Population Size", ylim= c(0,1))
abline(v=450,col="red")
abline(v=mean(estPopSize),col="black",lty=2)
mean(estPopSize)
sd(estPopSize)
```

For more information, such as how to use the recall, precision, and other summary statistics, please see the paper.
